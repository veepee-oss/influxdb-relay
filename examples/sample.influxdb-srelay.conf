###############################
##
## InfluxDB Single instances Config
##
###############################

# InfluxDB Backend InfluxDB01
[[influxdb]]
  name = "influxdb01"
  location = "http://127.0.0.1:8086/"
  timeout = "10s"

# InfluxDB Backend InfluxDB02
[[influxdb]]
  name = "influxdb02"
  location = "http://127.0.0.1:8087/"
  timeout = "10s"


#################################
##
## InfluxDB Cluster Configs as a set 
## of influxdb Single Instances
##
################################# 


# Cluster for linux Metrics
[[influxcluster]]
  # name = cluster id for route configs and logs
  name  = "cluster_linux"
  # members = array of influxdb backends
  members = ["influxdb01","influxdb02"]
  # where to write logs for all  operations in the cluster
  log-file = "cluster_linux.log"
  # log level could be
  # "panic","fatal","Error","warn","info","debug"
  log-level = "debug"
  # mode = of query and send data
  # * HA : 
  #       input write data will be sent to all members
  #       queries will be sent  on the active node with 
  #       data if query-router-endpoint-api config exist, else the first
  # * Single:
  #       input write data will be sent on the first member ( should be only one)
  #     query will be sent on  the only first member (should be only one)
  # * LB:  // NOT IMPLEMENTED YET //
  type = "HA"
  # the API to query on each input query
  #  rate-limit
  rate-limit = 100  
  burst-limit =  100
  default-ping-response-code = 200

# Cluster for WeblogicServer Metrics
[[influxcluster]]
  name  = "cluster_windows"
  members = ["influxdb01","influxdb02"]
  log-file = "cluster_windows.log"
  log-level = "info"
  type = "HA"
  rate-limit = 1000
  burst-limit =  1000
  query-router-endpoint-api = ["http://127.0.0.1:4090/api/queryactive","http://127.0.0.1:4090/api/queryactive"]

# Cluster for Container As A Service Metrics
[[influxcluster]]
  name  = "cluster_caas"
  members = ["influxdb01","influxdb02"]
  log-file = "cluster_caas.log"
  log-level = "debug"
  type = "HA"
  rate-limit = 1000
  burst-limit =  1000

# HTTP Server
[[http]]
  name = "example-http-influxdb2"
  bind-addr = "0.0.0.0:9096"
  log-file = "http_relay_9096.log"
  log-level = "debug"
  access-log = "access.log"

  rate-limit = 1000000
  burst-limit = 2000000

  # TLS
  # tls_cert = "/etc/influxdb-srelay/cert.pem"
  # tls_key = "/etc/influxdb-srelay/key.pem"

  ## Define endpoints base config
  ## endpoints can have read and write properties

  ## Example: /query endpoint
  ## There are 2 routes that must do a read query  against a cluster
  ## It expects that each HTTP request tries each route. If it fits the filter it will be enrouted
  ## All requests that doesn't pass through  the filter tries the next route


  #
  # IQL /query Endpoint
  #
  [[http.endpoint]]
    uri=["/query"]
    # type
    #  * RD = http for query db
    #  * WR = http for send data to the db
    type="RD"
    # source_format 
    # Supported formats
    # * ILP = influxdb-line-protocol
    # * prom-write = prometheus write format
    # * IQL = Influx Query Language
    source_format="IQL"

    ## READ request - linux_METRICS
    [[http.endpoint.route]]
      name="linux_metrics"
      # level:
      #   * http => all following rules will work only with http params
      #   * data => any of the following rules will need data inspection
      level="http" # http or data
      #log-inhit 
      # true => will use the endpoint log as this route log
      # false => will use its own log file ,  if not set the name <logdir>/http_route_<route_name>.log
      log-inherit = false
      #log-file = "query_route_linux_metrics.log"
      log-level = "debug"

      ## Filter only the request with db param = linux_metrics
      [[http.endpoint.route.filter]]
        name="pass_linux_metrics"
        #------------------------------------------------------------------------------------
        # Key for filter usage could be only at http level parameters (header/query)
        #  Header based
        #  -------------
        #    * authorization: Authorization Header 
        #    * remote-address: Remote Address Header
        #    * referer: Referer Header
        #    * user-agent: User-Agent Header
        #  Query Based 
        #  -------------
        #   (https://docs.influxdata.com/influxdb/v1.7/guides/querying_data/)
        #   (https://docs.influxdata.com/influxdb/v1.7/tools/api/#write-http-endpoint)
        #    * db [r/w]: InfluxDB to read / Write
        #    * q [r]: InfluxQL query
        #    * epoch [r]: precision on read queries
        #    * precision [w] : precision on write queries
        #    * chunked [r]: (see referenced doc)
        #    * chunksize[r]: (see referenced doc)
        #    * pretty[r]:(see referenced doc)
        #    * u [r/w]: read/write user
        #    * p [r/w]: read/write password
        #    * rp[w]: retention policy
        #    * consistency[w]: (see referenced doc)
        #  Computed
        #    * username: computed from authorization header or u parameters
        # Key for Rule Usage (not this level) could be also data level parameters
        #   * measurement: match the measurement name
        #   * tag: match the tag value with tag key in key_aux
        #   * field: match the field value with field key in key_aux (always as string!!!! at this level)
        # ----------------------------------------------------------------------------------------------
        key="db" #availabe http params
        match="linux_metrics"

      ## Rule to route to cluster_linux
      [[http.endpoint.route.rule]]
        name="linux_metrics"
        # Action Route
        # 	* route:
        #       If key value (usually http level key) match with match parameter, The complete 
        #       request will be sent to the cluster configured in to_cluster param
        #       After processed Next rule step will have untouched data available for any other process
        #
	      #   * route_db_from_data (enable multitenancy)
        #       Will rename de db parameter depending for each point in data depending on
        #       the matching with one point parameter , by example one tag, enable write data
        #       to several databases (split data) from the same source.
        #       with this rule 1 HTTP request will become N HTTP request to our backends
        #       HTTP response will be logged without feedback with the original request
        #       
        #       Rigth now only support for maches on key:
        #         * "measurement" : will send point to db = `value` parameter if measurement 
        #                           name matches with `match` parameter regex
        #         * "tagvalue": will send point do db = `value` parameter if point has a tag 
        #                         with tag name == key_aux and its value matches with `match` parameter regex
        #         * "tagname":       (Not supported yet)
        #         * "fieldvalue":    (Not supported yet)
        #         * "fieldname"      (Not supported yet)
        #       After processed, next rule step will have original  untouched data available for any other process
	      #   * rename_http (not yet supported)
	      #   * rename_data 
        #         If key value match with match parameter,some change will be done in the incomming point.
        #         Key values could be:
        #         * "measurement": will replace the measurement name with `value` parameter if match with `match` parameter
        #         * "tagvalue":     (Not supported yet) will replace tag value with `value` par√°meter on tags with tag name == key_aux, 
        #                       if tag value match with `match` parameter
        #         * "tagname":      (Not supported yet)
        #         * "fieldvalue":   (Not supported yet)
        #         * "fieldname":     (Not supported yet)
        #       After processed, data to the next process have been changed (and original no longer available )
	      #   * drop_data             (not yet supported)
        #   * break                 (not yet supported)
        #
        #
        action="route"
        # See 
        key="db"
        match="linux_metrics"
        to_cluster="cluster_linux"
  

      ## Query  request - METRICS
    [[http.endpoint.route]]
      name="telegraf_test"
      level="http" # http or data

      [[http.endpoint.route.filter]]
        name="telegraf_metrics_filter"
        key="db"
        match="telegraf"

      ## Simple rule that routes to influxcluster
      [[http.endpoint.route.rule]]
        name="telegraf_metrics_rule"
        action="route"
        key="db"
        match=".*"
        to_cluster="cluster_windows" #send_to = "cluster_windows"

      ## READ request - CAAS_METRICS
    [[http.endpoint.route]]
      name="caas_metrics"
      level="http" # http or data

      [[http.endpoint.route.filter]]
        name="pass_caas_metrics"
        key="remote-address"
        match="grafana-ip"

      ## Simple rule that routes to influxcluster
      [[http.endpoint.route.rule]]
        name="caas_metrics"
        action="route"
        key="db"
        match=".*"
        to_cluster="cluster_caas" #send_to = "caas_cluster"



  # 
  #
  # IQL /write Endpoint 

  [[http.endpoint]]
    uri=["/write"]
    source_format="ILP"
    type = "WR"
    
    ## WRITE request - windows
    [[http.endpoint.route]]
      name="windows_filter"
      level="http"

      [[http.endpoint.route.filter]]
        name="pass_windows_metrics"
        key="db"
        match="windows_metrics"

      ## Send to PMEREASP15 cluster
      [[http.endpoint.route.rule]]
        name="route_windows_metrics"
        action="route"
        key="db"
        match=".*"
        to_cluster="cluster_windows"


    
    
    ## WRITE request -  to linux_METRICS
    [[http.endpoint.route]]
      name="linux_filter"
      level="data"  # we can scan on measurements/tags/fields 

      ## match only points in database linux_metrics
      [[http.endpoint.route.filter]]
        name="pass_linux_metrics"
        key="db"
        match="linux_metrics"

      ## Send to linux cluster if measurement  match "linux_ISS.*"
      [[http.endpoint.route.rule]]
        name="route_linux_metrics"
        action="route"
        key="measurement"
        match="cpu.*"
        to_cluster="cluster_linux"

      ## Send to linux cluster if measurement match "linux_statistics"
      [[http.endpoint.route.rule]]
        name="route_linux_metrics"
        action="route"
        key="measurement"
        match="linux_statistics"
        to_cluster="cluster_linux"

      ## Any other measurements in 

    # TELEGRAF TO CLUSTER windows
    [[http.endpoint.route]]
      name="telegraf"
      level="data"
      log-inherit = false
      log-level = "debug"

      [[http.endpoint.route.filter]]
        name="pass_all_tele"
        key="db"
        match="tele.*"

      ## Replace measurement cpu to cpu_2 
      [[http.endpoint.route.rule]]
        name="rename_mem_measurement_to_mem_2"
        action="rename_data"
        key="measurement"
        match="mem"
        value="mem_2"

      ## Replace measurement cpu to cpu_2 
      [[http.endpoint.route.rule]]
        name="to_namespace_metrics"
        action="route_db_from_data"
        key="tagvalue"
        key_aux="cpu"
        match="^cpu[0-9]*$" # be carefull , 'cpu[0-9]*' maches diferent than '^cpu[0-9]*$'
        value="cpu_db"      # CREATE DATABASE  cpu_db
        value_on_unmatch="test_2" # CREATE  DATABASE test_2
        to_cluster="cluster_linux"

      ## Send to Cluster_linux cluster, 
      [[http.endpoint.route.rule]]
        name="route_all_data_to_cluster_linux"
        action="route"
        key="db"
        match="telegraf"
        to_cluster="cluster_linux"


    ## WRITE request - ALL that it doesn't match go to sinc!
    [[http.endpoint.route]]
      name="other_sinc"
      level="http"

      [[http.endpoint.route.filter]]
        name="pass_all"
        key="db"
        match=".*"

      ## Send to __SINC__ cluster, 
      [[http.endpoint.route.rule]]
        name="sinc_other"
        action="route"
        key="db"
        match=".*"
        to_cluster="__sinc__"
      #
    

  #
  #
  # Prometheus /api/v1/prom/write ENDPOINT

  [[http.endpoint]]
    uri=["/api/v1/prom/write"]
    source_format="prom-write"
    type = "WR"
        
    ## WRITE request - CAAS_METRICS
    [[http.endpoint.route]]
      name="caas_metrics"
      level="data" ## Only for Rules , filter should be processed at HTTP level (route will apply on all data)

      ## Filter only CAAS METRICS and let pass to the following config
      [[http.endpoint.route.filter]]
        name="pass_caas_metrics"
        key="db"
        match="caas_metrics"
 
      ## Change database based on tag
      [[http.endpoint.route.rule]]
        name="to_namespace_metrics"
        action="route_db_from_data"
        key="tagvalue"
        key_aux="namespace"
        match="(.*)"
        value="$1"
        value_on_unmatch="test_1"
        to_cluster="cluster_caas"

      ## Change database based on tag
      [[http.endpoint.route.rule]]
        name="to_cluster_caas"
        action="route"
        key="db"
        match=".*"
        to_cluster="cluster_caas"

    ## WRITE request - ALL that it doesn't match go to sinc!
    [[http.endpoint.route]]
      name="other_sinc"
      level="http"

      [[http.endpoint.route.filter]]
        name="pass_all"
        key="db"
        match=".*"
            
      ## Send to PMEREASP15 cluster
      [[http.endpoint.route.rule]]
        name="sinc_other"
        action="route"
        key="db"
        match=".*"
        to_cluster="__sinc__"



  #####